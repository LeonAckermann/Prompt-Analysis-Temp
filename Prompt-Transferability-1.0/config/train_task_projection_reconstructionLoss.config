[train] #train parameters
epoch = 100
batch_size = 16

reader_num = 0

optimizer = adamw
learning_rate = 0.001
weight_decay = 0
step_size = 1
lr_multiplier = 1


[distributed]
use = True
backend = nccl

[eval] #eval parameters
batch_size = 64

reader_num = 0

[dataset] #data parameters
#dataset = IMDB,laptop,MNLI,MRPC,QNLI,QQP,restaurant,RTE,SST2,STSB,WNLI 
dataset = IMDB,laptop,MNLI,MRPC,QNLI,QQP,restaurant,RTE,SST2,WNLI 
#dataset = IMDB,laptop 
#dataset = agnewsPromptBert_mlm_s2,IMDBPromptBert_mlm_s1,IMDBPromptRoberta_mlm_s2,sciercPromptRoberta_mlm_s1,agnewsPromptRoberta_mlm_s2,IMDBPromptBert_mlm_s2,cs_wikiPromptRoberta_mlm_s1,sciercPromptBert_mlm_s2,sciercPromptBert_mlm_s1,sciercPromptRoberta_mlm_s2,cs_wikiPromptRoberta_mlm_s2,SST2PromptRoberta_mlm_s1,cs_wikiPromptBert_mlm_s2,agnewsPromptBert_mlm_s1,SST2PromptBert_mlm_s2,agnewsPromptRoberta_mlm_s1,SST2PromptBert_mlm_s1,cs_wikiPromptBert_mlm_s1,SST2PromptRoberta_mlm_s2,IMDBPromptRoberta_mlm_s1

[model] #model parameters
model_type = Roberta
model_size = base

tqdm_ncols = 150
